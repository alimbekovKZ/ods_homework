{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия №3\n",
    "<center>Автор материала: программист-исследователь Mail.Ru Group Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашнее задание № 8\n",
    "## <center> Vowpal Wabbit в задаче классификации тегов вопросов на Stackoverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План\n",
    "    1. Введение\n",
    "    2. Описание данных\n",
    "    3. Предобработка данных\n",
    "    4. Обучение и проверка моделей\n",
    "    5. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Введение\n",
    "\n",
    "В этом задании вы будете делать примерно то же, что я каждую неделю –  в Mail.Ru Group: обучать модели на выборке в несколько гигабайт. Задание можно выполнить и на Windows с Python, но я рекомендую поработать под \\*NIX-системой (например, через Docker) и активно использовать язык bash.\n",
    "Немного снобизма (простите, но правда): если вы захотите работать в лучших компаниях мира в области ML, вам все равно понадобится опыт работы с bash под UNIX.\n",
    "\n",
    "[Веб-форма](https://docs.google.com/forms/d/1VaxYXnmbpeP185qPk2_V_BzbeduVUVyTdLPQwSCxDGA/edit) для ответов.\n",
    "\n",
    "Для выполнения задания понадобится установленный Vowpal Wabbit (уже есть в докер-контейнере курса, см. инструкцию в Wiki [репозитория](https://github.com/Yorko/mlcourse_open) нашего курса) и примерно 70 Гб дискового пространства. Я тестировал решение не на каком-то суперкомпе, а на Macbook Pro 2015 (8 ядер, 16 Гб памяти), и самая тяжеловесная модель обучалась около 12 минут, так что задание реально выполнить и с простым железом. Но если вы планируете когда-либо арендовать сервера Amazon, можно попробовать это сделать уже сейчас.\n",
    "\n",
    "Материалы в помощь:\n",
    " - интерактивный [тьюториал](https://www.codecademy.com/en/courses/learn-the-command-line/lessons/environment/exercises/bash-profile) CodeAcademy по утилитам командной строки UNIX (примерно на час-полтора)\n",
    " - [статья](https://habrahabr.ru/post/280562/) про то, как арендовать на Amazon машину (еще раз: это не обязательно для выполнения задания, но будет хорошим опытом, если вы это делаете впервые)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Описание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеются 10 Гб вопросов со StackOverflow – [скачайте](https://drive.google.com/file/d/1ZU4J3KhJDrHVMj48fROFcTsTZKorPGlG/view) и распакуйте архив. \n",
    "\n",
    "Формат данных простой:<br>\n",
    "<center>*текст вопроса* (слова через пробел) TAB *теги вопроса* (через пробел)\n",
    "\n",
    "Здесь TAB – это символ табуляции.\n",
    "Пример первой записи в выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is there a way to apply a background color through css at the tr level i can apply it at the td level like this my td background color e8e8e8 background e8e8e8 however the background color doesn t seem to get applied when i attempt to apply the background color at the tr level like this my tr background color e8e8e8 background e8e8e8 is there a css trick to making this work or does css not natively support this for some reason \tcss css3 css-selectors\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь у нас текст вопроса, затем табуляция и теги вопроса: *css, css3* и *css-selectors*. Всего в выборке таких вопросов 10 миллионов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000 data/stackoverflow.10kk.tsv\n",
      "CPU times: user 2.67 s, sys: 566 ms, total: 3.24 s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l data/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на то, что такие данные я уже не хочу загружать в оперативную память и, пока можно, буду пользоваться эффективными утилитами UNIX –  head, tail, wc, cat, cut и прочими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте выберем в наших данных все вопросы с тегами *javascript, java, python, ruby, php, c++, c#, go, scala* и  *swift* и подготовим обучающую выборку в формате Vowpal Wabbit. Будем решать задачу 10-классовой классификации вопросов по перечисленным тегам.\n",
    "\n",
    "Вообще, как мы видим, у каждого вопроса может быть несколько тегов, но мы упростим себе задачу и будем у каждого вопроса выбирать один из перечисленных тегов либо игнорировать вопрос, если таковых тегов нет. \n",
    "Но вообще VW поддерживает multilabel classification (аргумент  --multilabel_oaa).\n",
    "<br>\n",
    "<br>\n",
    "Реализуйте в виде отдельного файла `preprocess.py` код для подготовки данных. Он должен отобрать строки, в которых есть перечисленные теги, и переписать их в отдельный файл в формат Vowpal Wabbit. Детали:\n",
    " - скрипт должен работать с аргументами командной строки: с путями к файлам на входе и на выходе\n",
    " - строки обрабатываются по одной (можно использовать tqdm для подсчета числа итераций)\n",
    " - если табуляций в строке нет или их больше одной, считаем строку поврежденной и пропускаем\n",
    " - в противном случае смотрим, сколько в строке тегов из списка *javascript, java, python, ruby, php, c++, c#, go, scala* и  *swift*. Если ровно один, то записываем строку в выходной файл в формате VW: `label | text`, где `label` – число от 1 до 10 (1 - *javascript*, ... 10 – *swift*). Пропускаем те строки, где интересующих тегов больше или меньше одного \n",
    " - из текста вопроса надо выкинуть двоеточия и вертикальные палки, если они есть – в VW это спецсимволы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Должно получиться вот такое число строк – 4389054. 10 Гб у меня обработались примерно за 2 минуты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 10000000/10000000 [06:36<00:00, 25195.37it/s]\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py data/stackoverflow.10kk.tsv data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4389054 data/stackoverflow.vw\n",
      "CPU times: user 1.31 s, sys: 289 ms, total: 1.6 s\n",
      "Wall time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l data/stackoverflow.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gzip data/stackoverflow.10kk.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поделите выборку на обучающую, проверочную и тестовую части в равной пропорции - по  1463018 в каждый файл. Перемешивать не надо, первые 1463018 строк должны пойти в обучающую часть `stackoverflow_train.vw`, последние 1463018 – в тестовую `stackoverflow_test.vw`, оставшиеся – в проверочную `stackoverflow_valid.vw`. \n",
    "\n",
    "Также сохраните векторы ответов для проверочной и тестовой выборки в отдельные файлы `stackoverflow_valid_labels.txt` и `stackoverflow_test_labels.txt`.\n",
    "\n",
    "Тут вам помогут утилиты `head`, `tail`, `split`, `cat` и `cut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.16 s, sys: 1.25 s, total: 7.41 s\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!head -n 1463018 data/stackoverflow.vw > data/stackoverflow_train.vw\n",
    "!head -n 2926036 data/stackoverflow.vw | tail -n 1463018 > data/stackoverflow_valid.vw\n",
    "!tail -n 1463018 data/stackoverflow.vw > data/stackoverflow_test.vw\n",
    "\n",
    "!cat data/stackoverflow_valid.vw | cut -f1 -d '|' > data/stackoverflow_valid_labels.txt\n",
    "!cat data/stackoverflow_test.vw | cut -f1 -d '|' > data/stackoverflow_test_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463018 data/stackoverflow_train.vw\n",
      "1463018 data/stackoverflow_valid.vw\n",
      "1463018 data/stackoverflow_test.vw\n",
      "1463018 data/stackoverflow_valid_labels.txt\n",
      "1463018 data/stackoverflow_valid_test.txt\n",
      "CPU times: user 1.79 s, sys: 365 ms, total: 2.15 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wc -l data/stackoverflow_train.vw\n",
    "!wc -l data/stackoverflow_valid.vw\n",
    "!wc -l data/stackoverflow_test.vw\n",
    "!wc -l data/stackoverflow_valid_labels.txt\n",
    "!wc -l data/stackoverflow_valid_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Обучение и проверка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите Vowpal Wabbit на выборке `stackoverflow_train.vw` 9 раз, перебирая параметры passes (1,3,5), ngram (1,2,3).\n",
    "Остальные параметры укажите следующие: `loss_function=hinge`, `bit_precision`=28 и `seed`=17. Также скажите VW, что это 10-классовая задача.\n",
    "\n",
    "Проверяйте долю правильных ответов на выборке `stackoverflow_valid.vw`. Выберите лучшую модель и проверьте качество на выборке `stackoverflow_test.vw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e8241267264ff9b04920633107529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn: passes=1; ngram=1\n",
      "\n",
      "learn: passes=1; ngram=1\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        7        7      209\n",
      "0.781250 0.812500           32           32.0        7        2      174\n",
      "0.765625 0.750000           64           64.0        3        3      204\n",
      "0.648438 0.531250          128          128.0        1        5       29\n",
      "0.609375 0.570312          256          256.0        5        1      169\n",
      "0.548828 0.488281          512          512.0        2        2      303\n",
      "0.456055 0.363281         1024         1024.0        3        3      123\n",
      "0.375000 0.293945         2048         2048.0        1        5       83\n",
      "0.309082 0.243164         4096         4096.0        1        1       79\n",
      "0.261841 0.214600         8192         8192.0        2        2      112\n",
      "0.220825 0.179810        16384        16384.0        7        7      252\n",
      "0.186096 0.151367        32768        32768.0        4        5      134\n",
      "0.159149 0.132202        65536        65536.0        5        5      145\n",
      "0.138329 0.117508       131072       131072.0        7        2      255\n",
      "0.120888 0.103447       262144       262144.0        7        7      101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/voudy/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/voudy/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/voudy/anaconda3/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108549 0.096210       524288       524288.0        1        1      818\n",
      "0.099817 0.091085      1048576      1048576.0        1        1      571\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.095764\n",
      "total feature number = 291954690\n",
      "test on valid...\n",
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        2        2      178\n",
      "0.000000 0.000000            2            2.0        7        7       74\n",
      "0.000000 0.000000            4            4.0        5        5      259\n",
      "0.000000 0.000000            8            8.0        7        7      144\n",
      "0.000000 0.000000           16           16.0        6        6      359\n",
      "0.000000 0.000000           32           32.0        2        2      400\n",
      "0.062500 0.125000           64           64.0        5        5     1061\n",
      "0.062500 0.062500          128          128.0        2        2      132\n",
      "0.078125 0.093750          256          256.0        6        6       88\n",
      "0.074219 0.070312          512          512.0        7        6       35\n",
      "0.069336 0.064453         1024         1024.0        1        1       88\n",
      "0.078125 0.086914         2048         2048.0        2        2      243\n",
      "0.079834 0.081543         4096         4096.0        2        2      263\n",
      "0.080933 0.082031         8192         8192.0        7        7      272\n",
      "0.082642 0.084351        16384        16384.0        2        2      127\n",
      "0.081848 0.081055        32768        32768.0        6        6       91\n",
      "0.083405 0.084961        65536        65536.0        5        5      286\n",
      "0.082672 0.081940       131072       131072.0        5        5      176\n",
      "0.082859 0.083046       262144       262144.0        2        2      248\n",
      "0.082989 0.083118       524288       524288.0        7        7       36\n",
      "0.083456 0.083923      1048576      1048576.0        1        1      110\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.083337\n",
      "total feature number = 291768232\n",
      "accuracy=0.9166626794748937\n",
      "learn: passes=1; ngram=2\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.750000 0.718750           64           64.0        3        3      406\n",
      "0.648438 0.546875          128          128.0        1        7       56\n",
      "0.617188 0.585938          256          256.0        5        1      336\n",
      "0.548828 0.480469          512          512.0        2        2      604\n",
      "0.454102 0.359375         1024         1024.0        3        3      244\n",
      "0.375000 0.295898         2048         2048.0        1        5      164\n",
      "0.306396 0.237793         4096         4096.0        1        1      156\n",
      "0.254761 0.203125         8192         8192.0        2        2      222\n",
      "0.211426 0.168091        16384        16384.0        7        7      502\n",
      "0.176117 0.140808        32768        32768.0        4        5      266\n",
      "0.147873 0.119629        65536        65536.0        5        5      288\n",
      "0.126411 0.104950       131072       131072.0        7        2      508\n",
      "0.108402 0.090393       262144       262144.0        7        7      200\n",
      "0.097092 0.085781       524288       524288.0        1        1     1634\n",
      "0.086005 0.074919      1048576      1048576.0        1        1     1140\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.081809\n",
      "total feature number = 580983344\n",
      "test on valid...\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.125000 0.125000           16           16.0        6        6      716\n",
      "0.062500 0.000000           32           32.0        2        2      798\n",
      "0.062500 0.062500           64           64.0        5        5     2120\n",
      "0.046875 0.031250          128          128.0        2        2      262\n",
      "0.074219 0.101562          256          256.0        6        6      174\n",
      "0.060547 0.046875          512          512.0        7        7       68\n",
      "0.056641 0.052734         1024         1024.0        1        1      174\n",
      "0.061523 0.066406         2048         2048.0        2        2      484\n",
      "0.062500 0.063477         4096         4096.0        2        2      524\n",
      "0.065918 0.069336         8192         8192.0        7        7      542\n",
      "0.067444 0.068970        16384        16384.0        2        2      252\n",
      "0.067230 0.067017        32768        32768.0        6        6      180\n",
      "0.068008 0.068787        65536        65536.0        5        5      570\n",
      "0.067581 0.067154       131072       131072.0        5        5      350\n",
      "0.067097 0.066612       262144       262144.0        2        2      494\n",
      "0.067423 0.067749       524288       524288.0        7        7       70\n",
      "0.067742 0.068062      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.067643\n",
      "total feature number = 580610428\n",
      "accuracy=0.932356949811964\n",
      "learn: passes=1; ngram=3\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        7        7      622\n",
      "0.781250 0.812500           32           32.0        7        2      517\n",
      "0.750000 0.718750           64           64.0        3        3      607\n",
      "0.648438 0.546875          128          128.0        1        7       82\n",
      "0.628906 0.609375          256          256.0        5        1      502\n",
      "0.564453 0.500000          512          512.0        2        2      904\n",
      "0.471680 0.378906         1024         1024.0        3        3      364\n",
      "0.400879 0.330078         2048         2048.0        1        5      244\n",
      "0.333252 0.265625         4096         4096.0        1        1      232\n",
      "0.281006 0.228760         8192         8192.0        2        2      331\n",
      "0.232361 0.183716        16384        16384.0        7        7      751\n",
      "0.192169 0.151978        32768        32768.0        4        5      397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.160690 0.129211        65536        65536.0        5        5      430\n",
      "0.136086 0.111481       131072       131072.0        7        2      760\n",
      "0.115993 0.095901       262144       262144.0        7        7      298\n",
      "0.102705 0.089417       524288       524288.0        1        1     2449\n",
      "0.090337 0.077969      1048576      1048576.0        1        1     1708\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.085885\n",
      "total feature number = 868548985\n",
      "test on valid...\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      529\n",
      "0.500000 0.000000            2            2.0        7        7      217\n",
      "0.250000 0.000000            4            4.0        5        5      772\n",
      "0.125000 0.000000            8            8.0        7        7      427\n",
      "0.062500 0.000000           16           16.0        6        6     1072\n",
      "0.031250 0.000000           32           32.0        2        2     1195\n",
      "0.046875 0.062500           64           64.0        5        5     3178\n",
      "0.046875 0.046875          128          128.0        2        2      391\n",
      "0.078125 0.109375          256          256.0        6        6      259\n",
      "0.064453 0.050781          512          512.0        7        7      100\n",
      "0.059570 0.054688         1024         1024.0        1        1      259\n",
      "0.063477 0.067383         2048         2048.0        2        2      724\n",
      "0.065186 0.066895         4096         4096.0        2        2      784\n",
      "0.067505 0.069824         8192         8192.0        7        7      811\n",
      "0.069336 0.071167        16384        16384.0        2        2      376\n",
      "0.070007 0.070679        32768        32768.0        6        2      268\n",
      "0.070435 0.070862        65536        65536.0        5        5      853\n",
      "0.070007 0.069580       131072       131072.0        5        5      523\n",
      "0.069561 0.069115       262144       262144.0        2        2      739\n",
      "0.069832 0.070103       524288       524288.0        7        7      103\n",
      "0.070187 0.070541      1048576      1048576.0        1        1      325\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.070030\n",
      "total feature number = 867989607\n",
      "accuracy=0.9299701028968885\n",
      "learn: passes=3; ngram=1\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_train.vw\n",
      "Error: need a cache file for multiple passes : try using --cache_file\n",
      "\n",
      "finished run\n",
      "number of examples = 0\n",
      "weighted example sum = 0.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = undefined (no holdout)\n",
      "total feature number = 0\n",
      "vw (parser.cc:668): need a cache file for multiple passes : try using --cache_file\n",
      "test on valid...\n",
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        1      178\n",
      "1.000000 1.000000            2            2.0        7        1       74\n",
      "1.000000 1.000000            4            4.0        5        1      259\n",
      "0.875000 0.750000            8            8.0        7        1      144\n",
      "0.812500 0.750000           16           16.0        6        1      359\n",
      "0.781250 0.750000           32           32.0        2        1      400\n",
      "0.781250 0.781250           64           64.0        5        1     1061\n",
      "0.796875 0.812500          128          128.0        2        1      132\n",
      "0.773438 0.750000          256          256.0        6        1       88\n",
      "0.791016 0.808594          512          512.0        7        1       35\n",
      "0.796875 0.802734         1024         1024.0        1        1       88\n",
      "0.793945 0.791016         2048         2048.0        2        1      243\n",
      "0.804199 0.814453         4096         4096.0        2        1      263\n",
      "0.797974 0.791748         8192         8192.0        7        1      272\n",
      "0.799927 0.801880        16384        16384.0        2        1      127\n",
      "0.797607 0.795288        32768        32768.0        6        1       91\n",
      "0.795715 0.793823        65536        65536.0        5        1      286\n",
      "0.794464 0.793213       131072       131072.0        5        1      176\n",
      "0.794910 0.795357       262144       262144.0        2        1      248\n",
      "0.796204 0.797497       524288       524288.0        7        1       36\n",
      "0.796605 0.797007      1048576      1048576.0        1        1      110\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.796756\n",
      "total feature number = 291768232\n",
      "accuracy=0.2032442526339389\n",
      "learn: passes=3; ngram=2\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_train.vw\n",
      "Error: need a cache file for multiple passes : try using --cache_file\n",
      "\n",
      "finished run\n",
      "number of examples = 0\n",
      "weighted example sum = 0.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = undefined (no holdout)\n",
      "total feature number = 0\n",
      "vw (parser.cc:668): need a cache file for multiple passes : try using --cache_file\n",
      "test on valid...\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        1      354\n",
      "1.000000 1.000000            2            2.0        7        1      146\n",
      "1.000000 1.000000            4            4.0        5        1      516\n",
      "0.875000 0.750000            8            8.0        7        1      286\n",
      "0.812500 0.750000           16           16.0        6        1      716\n",
      "0.781250 0.750000           32           32.0        2        1      798\n",
      "0.781250 0.781250           64           64.0        5        1     2120\n",
      "0.796875 0.812500          128          128.0        2        1      262\n",
      "0.773438 0.750000          256          256.0        6        1      174\n",
      "0.791016 0.808594          512          512.0        7        1       68\n",
      "0.796875 0.802734         1024         1024.0        1        1      174\n",
      "0.793945 0.791016         2048         2048.0        2        1      484\n",
      "0.804199 0.814453         4096         4096.0        2        1      524\n",
      "0.797974 0.791748         8192         8192.0        7        1      542\n",
      "0.799927 0.801880        16384        16384.0        2        1      252\n",
      "0.797607 0.795288        32768        32768.0        6        1      180\n",
      "0.795715 0.793823        65536        65536.0        5        1      570\n",
      "0.794464 0.793213       131072       131072.0        5        1      350\n",
      "0.794910 0.795357       262144       262144.0        2        1      494\n",
      "0.796204 0.797497       524288       524288.0        7        1       70\n",
      "0.796605 0.797007      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.069463\n",
      "total feature number = 580610428\n",
      "accuracy=0.9305374233263022\n",
      "learn: passes=3; ngram=3\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = data/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        2        7      472\n",
      "0.718750 0.687500           32           32.0        1        7     1207\n",
      "0.687500 0.656250           64           64.0        7        7      304\n",
      "0.640625 0.593750          128          128.0        5        1      823\n",
      "0.625000 0.609375          256          256.0        1        1      301\n",
      "0.544922 0.464844          512          512.0        2        1      199\n",
      "0.466797 0.388672         1024         1024.0        1        1      391\n",
      "0.402344 0.337891         2048         2048.0        7        7      208\n",
      "0.328125 0.253906         4096         4096.0        2        2      952\n",
      "0.279419 0.230713         8192         8192.0        5        2       67\n",
      "0.232727 0.186035        16384        16384.0        3        3     1738\n",
      "0.191132 0.149536        32768        32768.0        3        3       79\n",
      "0.159424 0.127716        65536        65536.0        4        4      547\n",
      "0.135880 0.112335       131072       131072.0        2        2      280\n",
      "0.117374 0.098869       262144       262144.0        5        5      691\n",
      "0.102125 0.086876       524288       524288.0        6        6      421\n",
      "0.090308 0.078491      1048576      1048576.0        1        1     1261\n",
      "0.081231 0.081231      2097152      2097152.0        5        5     2083 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 3\n",
      "weighted example sum = 3950151.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073110 h\n",
      "total feature number = 2345071710\n",
      "test on valid...\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      529\n",
      "0.500000 0.000000            2            2.0        7        7      217\n",
      "0.250000 0.000000            4            4.0        5        5      772\n",
      "0.125000 0.000000            8            8.0        7        7      427\n",
      "0.125000 0.125000           16           16.0        6        6     1072\n",
      "0.062500 0.000000           32           32.0        2        2     1195\n",
      "0.062500 0.062500           64           64.0        5        5     3178\n",
      "0.046875 0.031250          128          128.0        2        2      391\n",
      "0.074219 0.101562          256          256.0        6        6      259\n",
      "0.064453 0.054688          512          512.0        7        7      100\n",
      "0.059570 0.054688         1024         1024.0        1        1      259\n",
      "0.067871 0.076172         2048         2048.0        2        2      724\n",
      "0.068848 0.069824         4096         4096.0        2        2      784\n",
      "0.071411 0.073975         8192         8192.0        7        7      811\n",
      "0.073181 0.074951        16384        16384.0        2        2      376\n",
      "0.072571 0.071960        32768        32768.0        6        2      268\n",
      "0.072769 0.072968        65536        65536.0        5        5      853\n",
      "0.072800 0.072830       131072       131072.0        5        5      523\n",
      "0.072182 0.071564       262144       262144.0        2        2      739\n",
      "0.072634 0.073086       524288       524288.0        7        7      103\n",
      "0.072803 0.072973      1048576      1048576.0        1        1      325\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.072746\n",
      "total feature number = 867989607\n",
      "accuracy=0.9272544835401888\n",
      "learn: passes=5; ngram=1\n",
      "Generating 1-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = data/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      161\n",
      "0.500000 1.000000            2            2.0        4        1       68\n",
      "0.750000 1.000000            4            4.0        7        1       88\n",
      "0.750000 0.750000            8            8.0        7        1       95\n",
      "0.750000 0.750000           16           16.0        2        7      159\n",
      "0.750000 0.750000           32           32.0        1        7      404\n",
      "0.703125 0.656250           64           64.0        7        7      103\n",
      "0.617188 0.531250          128          128.0        5        5      276\n",
      "0.593750 0.570312          256          256.0        1        1      102\n",
      "0.533203 0.472656          512          512.0        2        5       68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.457031 0.380859         1024         1024.0        1        1      132\n",
      "0.383301 0.309570         2048         2048.0        7        7       71\n",
      "0.310059 0.236816         4096         4096.0        2        2      319\n",
      "0.262085 0.214111         8192         8192.0        5        5       24\n",
      "0.223450 0.184814        16384        16384.0        3        3      581\n",
      "0.185547 0.147644        32768        32768.0        3        3       28\n",
      "0.158493 0.131439        65536        65536.0        4        4      184\n",
      "0.137115 0.115738       131072       131072.0        2        2       95\n",
      "0.121109 0.105103       262144       262144.0        5        5      232\n",
      "0.108404 0.095699       524288       524288.0        6        6      142\n",
      "0.100007 0.091610      1048576      1048576.0        1        1      422\n",
      "0.092582 0.092582      2097152      2097152.0        5        5      696 h\n",
      "0.088155 0.083728      4194304      4194304.0        1        1      216 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.083287 h\n",
      "total feature number = 1313790250\n",
      "test on valid...\n",
      "Generating 1-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        2        2      178\n",
      "0.000000 0.000000            2            2.0        7        7       74\n",
      "0.000000 0.000000            4            4.0        5        5      259\n",
      "0.000000 0.000000            8            8.0        7        7      144\n",
      "0.000000 0.000000           16           16.0        6        6      359\n",
      "0.000000 0.000000           32           32.0        2        2      400\n",
      "0.046875 0.093750           64           64.0        5        5     1061\n",
      "0.062500 0.078125          128          128.0        2        2      132\n",
      "0.078125 0.093750          256          256.0        6        6       88\n",
      "0.074219 0.070312          512          512.0        7        6       35\n",
      "0.064453 0.054688         1024         1024.0        1        1       88\n",
      "0.072754 0.081055         2048         2048.0        2        2      243\n",
      "0.078369 0.083984         4096         4096.0        2        2      263\n",
      "0.080688 0.083008         8192         8192.0        7        7      272\n",
      "0.081665 0.082642        16384        16384.0        2        2      127\n",
      "0.081329 0.080994        32768        32768.0        6        2       91\n",
      "0.082382 0.083435        65536        65536.0        5        5      286\n",
      "0.081711 0.081039       131072       131072.0        5        5      176\n",
      "0.081207 0.080704       262144       262144.0        2        2      248\n",
      "0.081455 0.081703       524288       524288.0        7        7       36\n",
      "0.082036 0.082617      1048576      1048576.0        1        1      110\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.081973\n",
      "total feature number = 291768232\n",
      "accuracy=0.9180269825798453\n",
      "learn: passes=5; ngram=2\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = data/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        2        7      316\n",
      "0.718750 0.687500           32           32.0        1        7      806\n",
      "0.703125 0.687500           64           64.0        7        7      204\n",
      "0.632812 0.562500          128          128.0        5        5      550\n",
      "0.609375 0.585938          256          256.0        1        1      202\n",
      "0.531250 0.453125          512          512.0        2        5      134\n",
      "0.458984 0.386719         1024         1024.0        1        1      262\n",
      "0.382812 0.306641         2048         2048.0        7        7      140\n",
      "0.305664 0.228516         4096         4096.0        2        2      636\n",
      "0.255249 0.204834         8192         8192.0        5        7       46\n",
      "0.215027 0.174805        16384        16384.0        3        3     1160\n",
      "0.177612 0.140198        32768        32768.0        3        3       54\n",
      "0.148590 0.119568        65536        65536.0        4        4      366\n",
      "0.127281 0.105972       131072       131072.0        2        2      188\n",
      "0.110416 0.093552       262144       262144.0        5        5      462\n",
      "0.096605 0.082794       524288       524288.0        6        6      282\n",
      "0.086320 0.076035      1048576      1048576.0        1        1      842\n",
      "0.077806 0.077806      2097152      2097152.0        5        5     1390 h\n",
      "0.073817 0.069828      4194304      4194304.0        1        1      430 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.069835 h\n",
      "total feature number = 2614413330\n",
      "test on valid...\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      354\n",
      "0.500000 0.000000            2            2.0        7        7      146\n",
      "0.250000 0.000000            4            4.0        5        5      516\n",
      "0.125000 0.000000            8            8.0        7        7      286\n",
      "0.125000 0.125000           16           16.0        6        6      716\n",
      "0.062500 0.000000           32           32.0        2        2      798\n",
      "0.062500 0.062500           64           64.0        5        5     2120\n",
      "0.054688 0.046875          128          128.0        2        2      262\n",
      "0.078125 0.101562          256          256.0        6        6      174\n",
      "0.068359 0.058594          512          512.0        7        7       68\n",
      "0.061523 0.054688         1024         1024.0        1        1      174\n",
      "0.065430 0.069336         2048         2048.0        2        2      484\n",
      "0.065186 0.064941         4096         4096.0        2        2      524\n",
      "0.068359 0.071533         8192         8192.0        7        7      542\n",
      "0.069519 0.070679        16384        16384.0        2        2      252\n",
      "0.069061 0.068604        32768        32768.0        6        2      180\n",
      "0.069504 0.069946        65536        65536.0        5        5      570\n",
      "0.068832 0.068161       131072       131072.0        5        5      350\n",
      "0.068092 0.067352       262144       262144.0        2        2      494\n",
      "0.068642 0.069191       524288       524288.0        7        7       70\n",
      "0.068837 0.069033      1048576      1048576.0        1        1      218\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.068765\n",
      "total feature number = 580610428\n",
      "accuracy=0.93123529580634\n",
      "learn: passes=5; ngram=3\n",
      "Generating 3-grams for all namespaces.\n",
      "final_regressor = data/model/valid_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "using cache_file = data/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      478\n",
      "0.500000 1.000000            2            2.0        4        1      199\n",
      "0.750000 1.000000            4            4.0        7        1      259\n",
      "0.750000 0.750000            8            8.0        7        1      280\n",
      "0.750000 0.750000           16           16.0        2        7      472\n",
      "0.718750 0.687500           32           32.0        1        7     1207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.687500 0.656250           64           64.0        7        7      304\n",
      "0.640625 0.593750          128          128.0        5        1      823\n",
      "0.625000 0.609375          256          256.0        1        1      301\n",
      "0.544922 0.464844          512          512.0        2        1      199\n",
      "0.466797 0.388672         1024         1024.0        1        1      391\n",
      "0.402344 0.337891         2048         2048.0        7        7      208\n",
      "0.328125 0.253906         4096         4096.0        2        2      952\n",
      "0.279419 0.230713         8192         8192.0        5        2       67\n",
      "0.232727 0.186035        16384        16384.0        3        3     1738\n",
      "0.191132 0.149536        32768        32768.0        3        3       79\n",
      "0.159424 0.127716        65536        65536.0        4        4      547\n",
      "0.135880 0.112335       131072       131072.0        2        2      280\n",
      "0.117374 0.098869       262144       262144.0        5        5      691\n",
      "0.102125 0.086876       524288       524288.0        6        6      421\n",
      "0.090308 0.078491      1048576      1048576.0        1        1     1261\n",
      "0.081231 0.081231      2097152      2097152.0        5        5     2083 h\n",
      "0.077415 0.073600      4194304      4194304.0        1        1      643 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1316717\n",
      "passes used = 5\n",
      "weighted example sum = 6583585.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073110 h\n",
      "total feature number = 3908452850\n",
      "test on valid...\n",
      "Generating 3-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/valid_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        7      529\n",
      "0.500000 0.000000            2            2.0        7        7      217\n",
      "0.250000 0.000000            4            4.0        5        5      772\n",
      "0.125000 0.000000            8            8.0        7        7      427\n",
      "0.125000 0.125000           16           16.0        6        6     1072\n",
      "0.062500 0.000000           32           32.0        2        2     1195\n",
      "0.062500 0.062500           64           64.0        5        5     3178\n",
      "0.054688 0.046875          128          128.0        2        2      391\n",
      "0.078125 0.101562          256          256.0        6        6      259\n",
      "0.066406 0.054688          512          512.0        7        7      100\n",
      "0.060547 0.054688         1024         1024.0        1        1      259\n",
      "0.065430 0.070312         2048         2048.0        2        2      724\n",
      "0.065918 0.066406         4096         4096.0        2        2      784\n",
      "0.068970 0.072021         8192         8192.0        7        7      811\n",
      "0.071533 0.074097        16384        16384.0        2        2      376\n",
      "0.071381 0.071228        32768        32768.0        6        2      268\n",
      "0.071808 0.072235        65536        65536.0        5        5      853\n",
      "0.071968 0.072128       131072       131072.0        5        5      523\n",
      "0.071392 0.070816       262144       262144.0        2        2      739\n",
      "0.071695 0.071999       524288       524288.0        7        7      103\n",
      "0.071869 0.072042      1048576      1048576.0        1        1      325\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.071854\n",
      "total feature number = 867989607\n",
      "accuracy=0.9281464752996887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passes = [1, 3, 5]\n",
    "ngram = [1, 2, 3]\n",
    "\n",
    "params = [(p, n) for p in passes for n in ngram]\n",
    "\n",
    "with open('data/stackoverflow_valid_labels.txt', 'r') as file:\n",
    "    valid_labels = [int(label.strip()) for label in file]\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "for (p, n) in tqdm_notebook(params):\n",
    "    print('learn: passes={}; ngram={}'.format(p, n))\n",
    "    !vw --oaa 10 -d data/stackoverflow_train.vw --loss_function=hinge --bit_precision 28 --random_seed 17 \\\n",
    "        -f data/model/valid_model.vw --cache --passes $p --ngram $n \n",
    "    print('test on valid...')\n",
    "    !vw -i data/model/valid_model.vw -t -d data/stackoverflow_valid.vw \\\n",
    "        -p data/model/valid_predictions.txt\n",
    "    with open('data/model/valid_predictions.txt', 'r') as file:\n",
    "        test_labels = [int(label.strip()) for label in file]\n",
    "    acc = accuracy_score(valid_labels, test_labels)\n",
    "    print('accuracy={}'.format(acc))\n",
    "    if acc > best_score:\n",
    "        best_params = (p, n)\n",
    "        best_score = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.932356949811964, (1, 2))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 1.</font> Какое сочетание параметров дает наибольшую долю правильных ответов на проверочной выборке `stackoverflow_valid.vw`?\n",
    "- Биграммы и 3 прохода по выборке\n",
    "- Триграммы и 5 проходов по выборке\n",
    "- Биграммы и 1 проход по выборке\n",
    "- Униграммы и 1 проход по выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте лучшую (по доле правильных ответов на валидации) модель на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = data/model/best_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = data/stackoverflow_train.vw.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.750000 0.718750           64           64.0        3        3      406\n",
      "0.648438 0.546875          128          128.0        1        7       56\n",
      "0.617188 0.585938          256          256.0        5        1      336\n",
      "0.548828 0.480469          512          512.0        2        2      604\n",
      "0.454102 0.359375         1024         1024.0        3        3      244\n",
      "0.375000 0.295898         2048         2048.0        1        5      164\n",
      "0.306396 0.237793         4096         4096.0        1        1      156\n",
      "0.254761 0.203125         8192         8192.0        2        2      222\n",
      "0.211426 0.168091        16384        16384.0        7        7      502\n",
      "0.176117 0.140808        32768        32768.0        4        5      266\n",
      "0.147873 0.119629        65536        65536.0        5        5      288\n",
      "0.126411 0.104950       131072       131072.0        7        2      508\n",
      "0.108402 0.090393       262144       262144.0        7        7      200\n",
      "0.097092 0.085781       524288       524288.0        1        1     1634\n",
      "0.086005 0.074919      1048576      1048576.0        1        1     1140\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.081809\n",
      "total feature number = 580983344\n"
     ]
    }
   ],
   "source": [
    "!vw --oaa 10 -d data/stackoverflow_train.vw --loss_function hinge --bit_precision 28 --random_seed 17 \\\n",
    "        -f data/model/best_model.vw --cache --passes 1 --ngram 2 --save_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/test_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        9        9      370\n",
      "0.000000 0.000000            2            2.0        1        1       62\n",
      "0.250000 0.500000            4            4.0        5        5      158\n",
      "0.250000 0.250000            8            8.0        2        2      116\n",
      "0.125000 0.000000           16           16.0        6        6       74\n",
      "0.093750 0.062500           32           32.0        1        1      324\n",
      "0.078125 0.062500           64           64.0        4        4       52\n",
      "0.078125 0.078125          128          128.0        7        7      250\n",
      "0.070312 0.062500          256          256.0        7        1      192\n",
      "0.070312 0.070312          512          512.0        1        1       70\n",
      "0.058594 0.046875         1024         1024.0        2        2      176\n",
      "0.065918 0.073242         2048         2048.0        2        2       70\n",
      "0.069336 0.072754         4096         4096.0        3        3      784\n",
      "0.067261 0.065186         8192         8192.0        4        4      312\n",
      "0.066956 0.066650        16384        16384.0        4        4      296\n",
      "0.067780 0.068604        32768        32768.0        1        1      490\n",
      "0.068161 0.068542        65536        65536.0        6        6      488\n",
      "0.067421 0.066681       131072       131072.0       10       10      540\n",
      "0.067314 0.067207       262144       262144.0        3        3     1316\n",
      "0.067661 0.068008       524288       524288.0        7        7      516\n",
      "0.067592 0.067522      1048576      1048576.0        7        6     1150\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.067465\n",
      "total feature number = 582313634\n"
     ]
    }
   ],
   "source": [
    "!vw -i data/model/best_model.vw -t -d data/stackoverflow_test.vw \\\n",
    "        -p data/model/test_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9325346646452743"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/stackoverflow_test_labels.txt', 'r') as file:\n",
    "    valid_labels = [int(label.strip()) for label in file]\n",
    "with open('data/model/test_predictions.txt', 'r') as file:\n",
    "        test_labels = [int(label.strip()) for label in file]\n",
    "test_score = accuracy_score(valid_labels, test_labels)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.017771483331030513"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(best_score - test_score) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 2.</font> Как соотносятся доли правильных ответов лучшей (по доле правильных ответов на валидации) модели на проверочной и на тестовой выборках? (здесь % – это процентный пункт, т.е., скажем, снижение с 50% до 40% – это на 10%, а не 20%).\n",
    "- На тестовой ниже примерно на 2%\n",
    "- На тестовой ниже примерно на 3%\n",
    "- Результаты почти одинаковы – отличаются меньше чем на 0.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите VW с параметрами, подобранными на проверочной выборке, теперь на объединении обучающей и проверочной выборок. Посчитайте долю правильных ответов на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = data/model/combine_model.vw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = .cache\n",
      "Reading datafile = \n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        1        1      320\n",
      "0.500000 1.000000            2            2.0        4        1      134\n",
      "0.750000 1.000000            4            4.0        7        1      174\n",
      "0.750000 0.750000            8            8.0        7        1      188\n",
      "0.750000 0.750000           16           16.0        7        7      416\n",
      "0.781250 0.812500           32           32.0        7        2      346\n",
      "0.750000 0.718750           64           64.0        3        3      406\n",
      "0.648438 0.546875          128          128.0        1        7       56\n",
      "0.617188 0.585938          256          256.0        5        1      336\n",
      "0.548828 0.480469          512          512.0        2        2      604\n",
      "0.454102 0.359375         1024         1024.0        3        3      244\n",
      "0.375000 0.295898         2048         2048.0        1        5      164\n",
      "0.306396 0.237793         4096         4096.0        1        1      156\n",
      "0.254761 0.203125         8192         8192.0        2        2      222\n",
      "0.211426 0.168091        16384        16384.0        7        7      502\n",
      "0.176117 0.140808        32768        32768.0        4        5      266\n",
      "0.147873 0.119629        65536        65536.0        5        5      288\n",
      "0.126411 0.104950       131072       131072.0        7        2      508\n",
      "0.108402 0.090393       262144       262144.0        7        7      200\n",
      "0.097092 0.085781       524288       524288.0        1        1     1634\n",
      "0.086005 0.074919      1048576      1048576.0        1        1     1140\n",
      "0.077170 0.068335      2097152      2097152.0        2        2      342\n",
      "\n",
      "finished run\n",
      "number of examples = 2926036\n",
      "weighted example sum = 2926036.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.073888\n",
      "total feature number = 1161593772\n"
     ]
    }
   ],
   "source": [
    "!cat data/stackoverflow_train.vw data/stackoverflow_valid.vw | \\\n",
    "    vw  -f data/model/combine_model.vw \\\n",
    "    --oaa 10 --loss_function hinge --bit_precision 28 --random_seed 17 \\\n",
    "    --cache --passes 1 --ngram 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = data/model/combine_predictions.txt\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = data/stackoverflow_test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        9        9      370\n",
      "0.000000 0.000000            2            2.0        1        1       62\n",
      "0.000000 0.000000            4            4.0        5        5      158\n",
      "0.000000 0.000000            8            8.0        2        2      116\n",
      "0.000000 0.000000           16           16.0        6        6       74\n",
      "0.031250 0.062500           32           32.0        1        1      324\n",
      "0.046875 0.062500           64           64.0        4        4       52\n",
      "0.046875 0.046875          128          128.0        7        7      250\n",
      "0.058594 0.070312          256          256.0        7        1      192\n",
      "0.068359 0.078125          512          512.0        1        1       70\n",
      "0.061523 0.054688         1024         1024.0        2        2      176\n",
      "0.066406 0.071289         2048         2048.0        2        2       70\n",
      "0.066406 0.066406         4096         4096.0        3        3      784\n",
      "0.064087 0.061768         8192         8192.0        4        4      312\n",
      "0.063416 0.062744        16384        16384.0        4        4      296\n",
      "0.063934 0.064453        32768        32768.0        1        1      490\n",
      "0.064438 0.064941        65536        65536.0        6        6      488\n",
      "0.063576 0.062714       131072       131072.0       10       10      540\n",
      "0.063545 0.063515       262144       262144.0        3        3     1316\n",
      "0.063738 0.063931       524288       524288.0        7        7      516\n",
      "0.063727 0.063717      1048576      1048576.0        7        6     1150\n",
      "\n",
      "finished run\n",
      "number of examples = 1463018\n",
      "weighted example sum = 1463018.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.063553\n",
      "total feature number = 582313634\n"
     ]
    }
   ],
   "source": [
    "!vw -i data/model/combine_model.vw -t -d data/stackoverflow_test.vw \\\n",
    "        -p data/model/combine_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9364471250524601"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/stackoverflow_test_labels.txt', 'r') as file:\n",
    "    valid_labels = [int(label.strip()) for label in file]\n",
    "with open('data/model/combine_predictions.txt', 'r') as file:\n",
    "    test_labels = [int(label.strip()) for label in file]\n",
    "combine_score = accuracy_score(valid_labels, test_labels)\n",
    "combine_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3912460407185736"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(combine_score - test_score) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Вопрос 3.</font> На сколько процентных пунктов повысилась доля правильных ответов модели после обучения на вдвое большей выборке (обучающая `stackoverflow_train.vw` + проверочная `stackoverflow_valid.vw`) по сравнению с моделью, обученной только на `stackoverflow_train.vw`?\n",
    " - 0.1%\n",
    " - 0.4%\n",
    " - 0.8%\n",
    " - 1.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы только познакомились с Vowpal Wabbit. Что еще можно попробовать:\n",
    " - Классификация с несколькими ответами (multilabel classification, аргумент  `multilabel_oaa`) – формат данных тут как раз под такую задачу\n",
    " - Настройка параметров VW с hyperopt, авторы библиотеки утверждают, что качество должно сильно зависеть от параметров изменения шага градиентного спуска (`initial_t` и `power_t`). Также можно потестировать разные функции потерь – обучать логистическую регресиию или линейный SVM\n",
    " - Познакомиться с факторизационными машинами и их реализацией в VW (аргумент `lrq`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
